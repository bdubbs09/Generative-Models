{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs: A simple examination of generative adversarial networks.\n",
    "Generative networks are great for generating synthetic data as well as image deblurring. The basis of the architecture is two neural nets playing a game where the generator tries to trick the discriminator. The discriminator sends feedback to the generator so that it can improve. The goal would be that the discriminator is 50/50 split between determining the fake or truth. That is ideal, but hardly practical in action.\n",
    "\n",
    "The networks are extremely difficult to train well. Some issues include vanishing gradients, inability to converge, instable gradients, hyperparameter tuning, as well as keeping a balance between the discriminator and the generator. The discriminator can hone in on the patterns the generator is using, and cause overfitting.\n",
    "\n",
    "In this notebook, I created a GAN for the MNIST data (boring, I know). It performs ok, but would be better with a designated GPU."
    "\n",
    "I need to include more of the math as well.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1600/1*5rMmuXmAquGTT-odw-bOpw.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://cdn-images-1.medium.com/max/1600/1*5rMmuXmAquGTT-odw-bOpw.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "## Why use Tensorboard?\n",
    "A neural net can be scary to the uninitiated. It can be hard to visualize what connects to what, what the flow of the algorithm is, as well as the gritty details and inner workings. Tensorboard can help with that. So we will create a graph to give a more intuitive look at what is going on inside the GAN, and specifically, what is going on within each network, as well as what it looks like. Tensorboard makes it much easier to trouble shoot your network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to figure out keras.callbacks.Tensorboard and if \n",
    "# I can find a solution to not using the .fit() since the GAN doesn't use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together.\n",
    "To make the project streamlined and simple, you can just reduce the entire thing to a single function call or even a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Main function body\n",
    "def GAN():\n",
    "    \n",
    "    # Recall the dimensions of the images\n",
    "    img_rows = 28\n",
    "    img_cols = 28\n",
    "    channels = 1\n",
    "    img_shape = (img_rows, img_cols, channels)\n",
    "    latent_dim = 100\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "    # Creation of generator\n",
    "    # Note the dimensions, and activation function\n",
    "    def generator():\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "    \n",
    "        model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "        \n",
    "        model.add(Reshape(img_shape))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    # Creation of generator\n",
    "    # Note the dimensions again, \n",
    "    # and activation function is sigmoid\n",
    "    def discriminator():\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "    \n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "        model.summary()       \n",
    "    \n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    " \n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Select a random batch of images\n",
    "            rnd = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[rnd]\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = gen.predict(noise)\n",
    "            \n",
    "            # Train the discriminator\n",
    "            d_loss_real = disc.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = disc.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = combined.train_on_batch(noise, valid)\n",
    "            \n",
    "            \n",
    "            # Show metrics and progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "    \n",
    "    \n",
    "    # Build and compile the discriminator\n",
    "    disc = discriminator()\n",
    "    disc.compile(loss='binary_crossentropy',\n",
    "                            optimizer=optimizer,\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "    # Build the generator\n",
    "    gen = generator()\n",
    "\n",
    "    # The generator takes noise as input and generates imgs\n",
    "    n = Input(shape=(latent_dim,))\n",
    "    img = gen(n)\n",
    "\n",
    "    # For the combined model we will only train the generator\n",
    "    # This is an important part\n",
    "    disc.trainable = False\n",
    "\n",
    "    # The discriminator takes generated images as input and determines validity\n",
    "    validity = disc(img)\n",
    "\n",
    "    # The combined model  (stacked generator and discriminator)\n",
    "    combined = Model(n, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return train(epochs=750, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.725809, acc.: 46.88%] [G loss: 0.746559]\n",
      "1 [D loss: 0.409508, acc.: 71.88%] [G loss: 0.759091]\n",
      "2 [D loss: 0.296316, acc.: 89.06%] [G loss: 0.928651]\n",
      "3 [D loss: 0.265069, acc.: 89.06%] [G loss: 1.135009]\n",
      "4 [D loss: 0.245505, acc.: 92.19%] [G loss: 1.255307]\n",
      "5 [D loss: 0.158225, acc.: 100.00%] [G loss: 1.473892]\n",
      "6 [D loss: 0.152748, acc.: 100.00%] [G loss: 1.517644]\n",
      "7 [D loss: 0.129214, acc.: 100.00%] [G loss: 1.670027]\n",
      "8 [D loss: 0.120396, acc.: 100.00%] [G loss: 1.785866]\n",
      "9 [D loss: 0.122549, acc.: 100.00%] [G loss: 1.915455]\n",
      "10 [D loss: 0.098941, acc.: 100.00%] [G loss: 1.922446]\n",
      "11 [D loss: 0.081969, acc.: 100.00%] [G loss: 1.965858]\n",
      "12 [D loss: 0.093209, acc.: 100.00%] [G loss: 2.092238]\n",
      "13 [D loss: 0.071329, acc.: 100.00%] [G loss: 2.110178]\n",
      "14 [D loss: 0.073641, acc.: 100.00%] [G loss: 2.241937]\n",
      "15 [D loss: 0.063215, acc.: 100.00%] [G loss: 2.262820]\n",
      "16 [D loss: 0.067612, acc.: 100.00%] [G loss: 2.363235]\n",
      "17 [D loss: 0.077806, acc.: 100.00%] [G loss: 2.454244]\n",
      "18 [D loss: 0.062706, acc.: 100.00%] [G loss: 2.543563]\n",
      "19 [D loss: 0.054604, acc.: 100.00%] [G loss: 2.575209]\n",
      "20 [D loss: 0.053023, acc.: 100.00%] [G loss: 2.638067]\n",
      "21 [D loss: 0.046362, acc.: 100.00%] [G loss: 2.659599]\n",
      "22 [D loss: 0.045245, acc.: 100.00%] [G loss: 2.872635]\n",
      "23 [D loss: 0.039006, acc.: 100.00%] [G loss: 2.827078]\n",
      "24 [D loss: 0.035042, acc.: 100.00%] [G loss: 2.794007]\n",
      "25 [D loss: 0.026728, acc.: 100.00%] [G loss: 2.885238]\n",
      "26 [D loss: 0.036500, acc.: 100.00%] [G loss: 2.952657]\n",
      "27 [D loss: 0.035371, acc.: 100.00%] [G loss: 2.876838]\n",
      "28 [D loss: 0.038938, acc.: 100.00%] [G loss: 2.977214]\n",
      "29 [D loss: 0.033177, acc.: 100.00%] [G loss: 3.163455]\n",
      "30 [D loss: 0.030759, acc.: 100.00%] [G loss: 3.101500]\n",
      "31 [D loss: 0.030507, acc.: 100.00%] [G loss: 3.251497]\n",
      "32 [D loss: 0.030185, acc.: 100.00%] [G loss: 3.255376]\n",
      "33 [D loss: 0.033654, acc.: 100.00%] [G loss: 3.311371]\n",
      "34 [D loss: 0.029939, acc.: 100.00%] [G loss: 3.272651]\n",
      "35 [D loss: 0.032124, acc.: 100.00%] [G loss: 3.395524]\n",
      "36 [D loss: 0.026654, acc.: 100.00%] [G loss: 3.453329]\n",
      "37 [D loss: 0.026890, acc.: 100.00%] [G loss: 3.460097]\n",
      "38 [D loss: 0.026808, acc.: 100.00%] [G loss: 3.510783]\n",
      "39 [D loss: 0.016813, acc.: 100.00%] [G loss: 3.512134]\n",
      "40 [D loss: 0.028043, acc.: 100.00%] [G loss: 3.692100]\n",
      "41 [D loss: 0.023152, acc.: 100.00%] [G loss: 3.820813]\n",
      "42 [D loss: 0.018753, acc.: 100.00%] [G loss: 3.529026]\n",
      "43 [D loss: 0.021719, acc.: 100.00%] [G loss: 3.576412]\n",
      "44 [D loss: 0.020922, acc.: 100.00%] [G loss: 3.696741]\n",
      "45 [D loss: 0.020141, acc.: 100.00%] [G loss: 3.646626]\n",
      "46 [D loss: 0.022953, acc.: 100.00%] [G loss: 3.653045]\n",
      "47 [D loss: 0.021874, acc.: 100.00%] [G loss: 3.797330]\n",
      "48 [D loss: 0.013709, acc.: 100.00%] [G loss: 3.856032]\n",
      "49 [D loss: 0.016289, acc.: 100.00%] [G loss: 3.893096]\n",
      "50 [D loss: 0.023745, acc.: 100.00%] [G loss: 3.921208]\n",
      "51 [D loss: 0.019705, acc.: 100.00%] [G loss: 3.816591]\n",
      "52 [D loss: 0.018192, acc.: 100.00%] [G loss: 3.803133]\n",
      "53 [D loss: 0.020770, acc.: 100.00%] [G loss: 3.896693]\n",
      "54 [D loss: 0.018726, acc.: 100.00%] [G loss: 4.046928]\n",
      "55 [D loss: 0.017238, acc.: 100.00%] [G loss: 3.843573]\n",
      "56 [D loss: 0.017112, acc.: 100.00%] [G loss: 4.077351]\n",
      "57 [D loss: 0.016878, acc.: 100.00%] [G loss: 3.922123]\n",
      "58 [D loss: 0.017439, acc.: 100.00%] [G loss: 4.100340]\n",
      "59 [D loss: 0.013234, acc.: 100.00%] [G loss: 4.148379]\n",
      "60 [D loss: 0.014565, acc.: 100.00%] [G loss: 4.102227]\n",
      "61 [D loss: 0.018023, acc.: 100.00%] [G loss: 4.014264]\n",
      "62 [D loss: 0.014200, acc.: 100.00%] [G loss: 4.099262]\n",
      "63 [D loss: 0.013862, acc.: 100.00%] [G loss: 4.094898]\n",
      "64 [D loss: 0.014626, acc.: 100.00%] [G loss: 4.156215]\n",
      "65 [D loss: 0.015823, acc.: 100.00%] [G loss: 4.180065]\n",
      "66 [D loss: 0.013829, acc.: 100.00%] [G loss: 4.231471]\n",
      "67 [D loss: 0.012083, acc.: 100.00%] [G loss: 4.124391]\n",
      "68 [D loss: 0.014468, acc.: 100.00%] [G loss: 4.231458]\n",
      "69 [D loss: 0.018276, acc.: 100.00%] [G loss: 4.248717]\n",
      "70 [D loss: 0.015982, acc.: 100.00%] [G loss: 4.405855]\n",
      "71 [D loss: 0.013015, acc.: 100.00%] [G loss: 4.307645]\n",
      "72 [D loss: 0.014054, acc.: 100.00%] [G loss: 4.282987]\n",
      "73 [D loss: 0.016037, acc.: 100.00%] [G loss: 4.239240]\n",
      "74 [D loss: 0.021254, acc.: 100.00%] [G loss: 4.479105]\n",
      "75 [D loss: 0.014813, acc.: 100.00%] [G loss: 4.480222]\n",
      "76 [D loss: 0.012956, acc.: 100.00%] [G loss: 4.461680]\n",
      "77 [D loss: 0.011522, acc.: 100.00%] [G loss: 4.561545]\n",
      "78 [D loss: 0.012942, acc.: 100.00%] [G loss: 4.432729]\n",
      "79 [D loss: 0.014607, acc.: 100.00%] [G loss: 4.233753]\n",
      "80 [D loss: 0.023833, acc.: 100.00%] [G loss: 4.495440]\n",
      "81 [D loss: 0.012630, acc.: 100.00%] [G loss: 4.446214]\n",
      "82 [D loss: 0.012925, acc.: 100.00%] [G loss: 4.595024]\n",
      "83 [D loss: 0.016670, acc.: 100.00%] [G loss: 4.523610]\n",
      "84 [D loss: 0.015934, acc.: 100.00%] [G loss: 4.600112]\n",
      "85 [D loss: 0.010214, acc.: 100.00%] [G loss: 4.632562]\n",
      "86 [D loss: 0.009703, acc.: 100.00%] [G loss: 4.619194]\n",
      "87 [D loss: 0.011761, acc.: 100.00%] [G loss: 4.635379]\n",
      "88 [D loss: 0.012237, acc.: 100.00%] [G loss: 4.763903]\n",
      "89 [D loss: 0.011328, acc.: 100.00%] [G loss: 4.488552]\n",
      "90 [D loss: 0.014487, acc.: 100.00%] [G loss: 4.531065]\n",
      "91 [D loss: 0.012731, acc.: 100.00%] [G loss: 4.529456]\n",
      "92 [D loss: 0.012025, acc.: 100.00%] [G loss: 4.626655]\n",
      "93 [D loss: 0.012387, acc.: 100.00%] [G loss: 4.611742]\n",
      "94 [D loss: 0.014002, acc.: 100.00%] [G loss: 4.705131]\n",
      "95 [D loss: 0.012755, acc.: 100.00%] [G loss: 4.687280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 [D loss: 0.016138, acc.: 100.00%] [G loss: 4.738323]\n",
      "97 [D loss: 0.015548, acc.: 100.00%] [G loss: 4.675002]\n",
      "98 [D loss: 0.012355, acc.: 100.00%] [G loss: 4.722579]\n",
      "99 [D loss: 0.014024, acc.: 100.00%] [G loss: 4.790741]\n",
      "100 [D loss: 0.020077, acc.: 100.00%] [G loss: 4.694950]\n",
      "101 [D loss: 0.021357, acc.: 100.00%] [G loss: 4.748833]\n",
      "102 [D loss: 0.012148, acc.: 100.00%] [G loss: 4.983460]\n",
      "103 [D loss: 0.022709, acc.: 100.00%] [G loss: 4.832717]\n",
      "104 [D loss: 0.008929, acc.: 100.00%] [G loss: 4.765514]\n",
      "105 [D loss: 0.013655, acc.: 100.00%] [G loss: 4.569659]\n",
      "106 [D loss: 0.010616, acc.: 100.00%] [G loss: 4.784124]\n",
      "107 [D loss: 0.029112, acc.: 100.00%] [G loss: 4.819283]\n",
      "108 [D loss: 0.022514, acc.: 100.00%] [G loss: 4.914570]\n",
      "109 [D loss: 0.014512, acc.: 100.00%] [G loss: 4.879535]\n",
      "110 [D loss: 0.015859, acc.: 100.00%] [G loss: 4.718773]\n",
      "111 [D loss: 0.014778, acc.: 100.00%] [G loss: 4.932926]\n",
      "112 [D loss: 0.041828, acc.: 98.44%] [G loss: 5.572593]\n",
      "113 [D loss: 0.606387, acc.: 75.00%] [G loss: 3.492711]\n",
      "114 [D loss: 0.420065, acc.: 82.81%] [G loss: 3.999687]\n",
      "115 [D loss: 0.038140, acc.: 100.00%] [G loss: 4.954152]\n",
      "116 [D loss: 0.028573, acc.: 100.00%] [G loss: 4.951169]\n",
      "117 [D loss: 0.038035, acc.: 98.44%] [G loss: 5.151870]\n",
      "118 [D loss: 0.023012, acc.: 100.00%] [G loss: 5.130136]\n",
      "119 [D loss: 0.019370, acc.: 100.00%] [G loss: 4.703381]\n",
      "120 [D loss: 0.036170, acc.: 100.00%] [G loss: 4.602012]\n",
      "121 [D loss: 0.052209, acc.: 100.00%] [G loss: 4.637981]\n",
      "122 [D loss: 0.027729, acc.: 100.00%] [G loss: 4.456985]\n",
      "123 [D loss: 0.061909, acc.: 100.00%] [G loss: 4.887970]\n",
      "124 [D loss: 0.050624, acc.: 100.00%] [G loss: 4.856286]\n",
      "125 [D loss: 0.074001, acc.: 96.88%] [G loss: 4.405150]\n",
      "126 [D loss: 0.055094, acc.: 98.44%] [G loss: 4.745433]\n",
      "127 [D loss: 0.087038, acc.: 98.44%] [G loss: 4.496117]\n",
      "128 [D loss: 0.092708, acc.: 98.44%] [G loss: 4.623208]\n",
      "129 [D loss: 0.042025, acc.: 98.44%] [G loss: 4.546791]\n",
      "130 [D loss: 0.111977, acc.: 96.88%] [G loss: 4.614387]\n",
      "131 [D loss: 0.112628, acc.: 95.31%] [G loss: 4.126230]\n",
      "132 [D loss: 0.073502, acc.: 98.44%] [G loss: 4.737287]\n",
      "133 [D loss: 0.232882, acc.: 90.62%] [G loss: 3.651536]\n",
      "134 [D loss: 0.105155, acc.: 96.88%] [G loss: 4.625235]\n",
      "135 [D loss: 0.211458, acc.: 93.75%] [G loss: 3.851757]\n",
      "136 [D loss: 0.067888, acc.: 98.44%] [G loss: 4.660020]\n",
      "137 [D loss: 0.382483, acc.: 85.94%] [G loss: 3.842935]\n",
      "138 [D loss: 0.098736, acc.: 95.31%] [G loss: 4.448216]\n",
      "139 [D loss: 0.331825, acc.: 82.81%] [G loss: 3.857965]\n",
      "140 [D loss: 0.057641, acc.: 98.44%] [G loss: 4.467547]\n",
      "141 [D loss: 0.247857, acc.: 92.19%] [G loss: 3.653394]\n",
      "142 [D loss: 0.117140, acc.: 93.75%] [G loss: 4.477437]\n",
      "143 [D loss: 0.374445, acc.: 82.81%] [G loss: 3.172399]\n",
      "144 [D loss: 0.200732, acc.: 93.75%] [G loss: 3.672893]\n",
      "145 [D loss: 0.070385, acc.: 96.88%] [G loss: 4.500117]\n",
      "146 [D loss: 0.227883, acc.: 90.62%] [G loss: 3.637276]\n",
      "147 [D loss: 0.107890, acc.: 95.31%] [G loss: 3.991961]\n",
      "148 [D loss: 0.118320, acc.: 96.88%] [G loss: 3.972193]\n",
      "149 [D loss: 0.141108, acc.: 96.88%] [G loss: 4.359563]\n",
      "150 [D loss: 0.274301, acc.: 87.50%] [G loss: 4.530123]\n",
      "151 [D loss: 0.742504, acc.: 70.31%] [G loss: 3.503603]\n",
      "152 [D loss: 0.091758, acc.: 98.44%] [G loss: 4.064771]\n",
      "153 [D loss: 0.178602, acc.: 93.75%] [G loss: 3.504858]\n",
      "154 [D loss: 0.158973, acc.: 90.62%] [G loss: 3.700378]\n",
      "155 [D loss: 0.135858, acc.: 98.44%] [G loss: 3.331404]\n",
      "156 [D loss: 0.241280, acc.: 87.50%] [G loss: 4.261003]\n",
      "157 [D loss: 1.112768, acc.: 57.81%] [G loss: 2.836890]\n",
      "158 [D loss: 0.324186, acc.: 84.38%] [G loss: 2.922437]\n",
      "159 [D loss: 0.140846, acc.: 93.75%] [G loss: 4.091435]\n",
      "160 [D loss: 0.100005, acc.: 100.00%] [G loss: 3.511466]\n",
      "161 [D loss: 0.142890, acc.: 92.19%] [G loss: 3.961213]\n",
      "162 [D loss: 0.159450, acc.: 92.19%] [G loss: 3.784712]\n",
      "163 [D loss: 0.147812, acc.: 98.44%] [G loss: 3.768357]\n",
      "164 [D loss: 0.167114, acc.: 92.19%] [G loss: 3.839596]\n",
      "165 [D loss: 0.236931, acc.: 93.75%] [G loss: 2.723249]\n",
      "166 [D loss: 0.124708, acc.: 96.88%] [G loss: 3.339832]\n",
      "167 [D loss: 0.096529, acc.: 95.31%] [G loss: 4.411716]\n",
      "168 [D loss: 0.390870, acc.: 81.25%] [G loss: 2.651415]\n",
      "169 [D loss: 0.247426, acc.: 84.38%] [G loss: 3.882829]\n",
      "170 [D loss: 0.223260, acc.: 92.19%] [G loss: 2.881294]\n",
      "171 [D loss: 0.169589, acc.: 93.75%] [G loss: 3.837958]\n",
      "172 [D loss: 0.252425, acc.: 89.06%] [G loss: 3.519793]\n",
      "173 [D loss: 0.141785, acc.: 93.75%] [G loss: 4.041943]\n",
      "174 [D loss: 0.391203, acc.: 82.81%] [G loss: 3.230949]\n",
      "175 [D loss: 0.138916, acc.: 95.31%] [G loss: 3.951216]\n",
      "176 [D loss: 0.259666, acc.: 89.06%] [G loss: 3.118147]\n",
      "177 [D loss: 0.096884, acc.: 96.88%] [G loss: 3.515099]\n",
      "178 [D loss: 0.206323, acc.: 92.19%] [G loss: 3.784086]\n",
      "179 [D loss: 0.160250, acc.: 96.88%] [G loss: 3.063576]\n",
      "180 [D loss: 0.212345, acc.: 92.19%] [G loss: 4.598736]\n",
      "181 [D loss: 1.130223, acc.: 54.69%] [G loss: 2.460371]\n",
      "182 [D loss: 0.291781, acc.: 85.94%] [G loss: 3.326354]\n",
      "183 [D loss: 0.049392, acc.: 100.00%] [G loss: 4.109317]\n",
      "184 [D loss: 0.214268, acc.: 89.06%] [G loss: 3.189669]\n",
      "185 [D loss: 0.201183, acc.: 89.06%] [G loss: 3.985306]\n",
      "186 [D loss: 0.143100, acc.: 98.44%] [G loss: 3.714087]\n",
      "187 [D loss: 0.215500, acc.: 90.62%] [G loss: 4.025882]\n",
      "188 [D loss: 0.269012, acc.: 87.50%] [G loss: 3.552292]\n",
      "189 [D loss: 0.125073, acc.: 95.31%] [G loss: 3.668685]\n",
      "190 [D loss: 0.169851, acc.: 93.75%] [G loss: 3.962414]\n",
      "191 [D loss: 0.190781, acc.: 92.19%] [G loss: 2.968019]\n",
      "192 [D loss: 0.109712, acc.: 96.88%] [G loss: 3.103622]\n",
      "193 [D loss: 0.202941, acc.: 87.50%] [G loss: 4.612542]\n",
      "194 [D loss: 2.069739, acc.: 28.12%] [G loss: 1.349822]\n",
      "195 [D loss: 1.544281, acc.: 54.69%] [G loss: 0.966135]\n",
      "196 [D loss: 0.764600, acc.: 57.81%] [G loss: 1.589918]\n",
      "197 [D loss: 0.131539, acc.: 96.88%] [G loss: 2.982601]\n",
      "198 [D loss: 0.145490, acc.: 96.88%] [G loss: 3.483322]\n",
      "199 [D loss: 0.191720, acc.: 98.44%] [G loss: 3.184348]\n",
      "200 [D loss: 0.120274, acc.: 100.00%] [G loss: 3.084304]\n",
      "201 [D loss: 0.174946, acc.: 95.31%] [G loss: 3.330186]\n",
      "202 [D loss: 0.143597, acc.: 95.31%] [G loss: 3.176338]\n",
      "203 [D loss: 0.188101, acc.: 96.88%] [G loss: 2.930673]\n",
      "204 [D loss: 0.133555, acc.: 100.00%] [G loss: 2.802397]\n",
      "205 [D loss: 0.182376, acc.: 95.31%] [G loss: 3.165248]\n",
      "206 [D loss: 0.165508, acc.: 96.88%] [G loss: 2.952103]\n",
      "207 [D loss: 0.224960, acc.: 93.75%] [G loss: 2.807664]\n",
      "208 [D loss: 0.147409, acc.: 98.44%] [G loss: 2.960630]\n",
      "209 [D loss: 0.252109, acc.: 92.19%] [G loss: 2.702404]\n",
      "210 [D loss: 0.218643, acc.: 93.75%] [G loss: 2.988749]\n",
      "211 [D loss: 0.210458, acc.: 92.19%] [G loss: 2.688244]\n",
      "212 [D loss: 0.206260, acc.: 98.44%] [G loss: 2.690934]\n",
      "213 [D loss: 0.251338, acc.: 90.62%] [G loss: 3.194176]\n",
      "214 [D loss: 0.289279, acc.: 92.19%] [G loss: 2.566686]\n",
      "215 [D loss: 0.255447, acc.: 87.50%] [G loss: 3.304831]\n",
      "216 [D loss: 0.302963, acc.: 87.50%] [G loss: 3.081838]\n",
      "217 [D loss: 0.258288, acc.: 90.62%] [G loss: 3.124596]\n",
      "218 [D loss: 0.243350, acc.: 90.62%] [G loss: 2.404059]\n",
      "219 [D loss: 0.262394, acc.: 85.94%] [G loss: 3.443310]\n",
      "220 [D loss: 0.614091, acc.: 67.19%] [G loss: 2.254084]\n",
      "221 [D loss: 0.228047, acc.: 90.62%] [G loss: 3.301276]\n",
      "222 [D loss: 0.309085, acc.: 90.62%] [G loss: 2.724288]\n",
      "223 [D loss: 0.221091, acc.: 93.75%] [G loss: 3.241486]\n",
      "224 [D loss: 0.417620, acc.: 75.00%] [G loss: 2.671871]\n",
      "225 [D loss: 0.229555, acc.: 90.62%] [G loss: 3.592498]\n",
      "226 [D loss: 0.512173, acc.: 71.88%] [G loss: 2.237286]\n",
      "227 [D loss: 0.208444, acc.: 90.62%] [G loss: 3.725095]\n",
      "228 [D loss: 0.583720, acc.: 76.56%] [G loss: 1.775522]\n",
      "229 [D loss: 0.350091, acc.: 81.25%] [G loss: 2.918719]\n",
      "230 [D loss: 0.165470, acc.: 96.88%] [G loss: 3.406199]\n",
      "231 [D loss: 0.396633, acc.: 84.38%] [G loss: 3.048243]\n",
      "232 [D loss: 0.286176, acc.: 87.50%] [G loss: 3.571831]\n",
      "233 [D loss: 0.929211, acc.: 43.75%] [G loss: 1.513538]\n",
      "234 [D loss: 0.281552, acc.: 84.38%] [G loss: 3.581545]\n",
      "235 [D loss: 0.757452, acc.: 53.12%] [G loss: 1.930218]\n",
      "236 [D loss: 0.243230, acc.: 92.19%] [G loss: 3.273131]\n",
      "237 [D loss: 0.576311, acc.: 70.31%] [G loss: 1.783738]\n",
      "238 [D loss: 0.251970, acc.: 92.19%] [G loss: 2.921795]\n",
      "239 [D loss: 0.304528, acc.: 87.50%] [G loss: 3.004488]\n",
      "240 [D loss: 0.467296, acc.: 71.88%] [G loss: 2.636230]\n",
      "241 [D loss: 0.301028, acc.: 84.38%] [G loss: 4.252819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242 [D loss: 1.204338, acc.: 31.25%] [G loss: 1.241551]\n",
      "243 [D loss: 0.352295, acc.: 79.69%] [G loss: 2.748218]\n",
      "244 [D loss: 0.321971, acc.: 92.19%] [G loss: 3.524937]\n",
      "245 [D loss: 0.875037, acc.: 50.00%] [G loss: 1.092059]\n",
      "246 [D loss: 0.339966, acc.: 81.25%] [G loss: 2.177796]\n",
      "247 [D loss: 0.269358, acc.: 92.19%] [G loss: 2.956503]\n",
      "248 [D loss: 0.725092, acc.: 60.94%] [G loss: 1.662487]\n",
      "249 [D loss: 0.427593, acc.: 71.88%] [G loss: 2.465637]\n",
      "250 [D loss: 0.650610, acc.: 57.81%] [G loss: 2.040071]\n",
      "251 [D loss: 0.475492, acc.: 73.44%] [G loss: 2.679103]\n",
      "252 [D loss: 0.957259, acc.: 35.94%] [G loss: 1.169075]\n",
      "253 [D loss: 0.427471, acc.: 73.44%] [G loss: 2.620695]\n",
      "254 [D loss: 0.838370, acc.: 46.88%] [G loss: 1.434933]\n",
      "255 [D loss: 0.397916, acc.: 79.69%] [G loss: 2.431951]\n",
      "256 [D loss: 0.638876, acc.: 68.75%] [G loss: 1.661821]\n",
      "257 [D loss: 0.549453, acc.: 64.06%] [G loss: 2.211671]\n",
      "258 [D loss: 0.694813, acc.: 56.25%] [G loss: 1.716542]\n",
      "259 [D loss: 0.591246, acc.: 60.94%] [G loss: 2.019099]\n",
      "260 [D loss: 0.677933, acc.: 60.94%] [G loss: 1.892950]\n",
      "261 [D loss: 0.439502, acc.: 84.38%] [G loss: 1.788569]\n",
      "262 [D loss: 0.685952, acc.: 60.94%] [G loss: 1.912423]\n",
      "263 [D loss: 0.636421, acc.: 70.31%] [G loss: 1.768904]\n",
      "264 [D loss: 0.664173, acc.: 54.69%] [G loss: 1.604453]\n",
      "265 [D loss: 0.535602, acc.: 68.75%] [G loss: 2.039671]\n",
      "266 [D loss: 1.006010, acc.: 42.19%] [G loss: 1.000717]\n",
      "267 [D loss: 0.503407, acc.: 68.75%] [G loss: 1.678258]\n",
      "268 [D loss: 0.753050, acc.: 51.56%] [G loss: 1.252817]\n",
      "269 [D loss: 0.527037, acc.: 70.31%] [G loss: 1.725788]\n",
      "270 [D loss: 0.826903, acc.: 43.75%] [G loss: 1.047292]\n",
      "271 [D loss: 0.589939, acc.: 60.94%] [G loss: 1.717671]\n",
      "272 [D loss: 0.610630, acc.: 64.06%] [G loss: 1.478330]\n",
      "273 [D loss: 0.542019, acc.: 70.31%] [G loss: 1.596159]\n",
      "274 [D loss: 0.681068, acc.: 56.25%] [G loss: 1.266356]\n",
      "275 [D loss: 0.585554, acc.: 65.62%] [G loss: 1.711767]\n",
      "276 [D loss: 0.637479, acc.: 59.38%] [G loss: 1.450772]\n",
      "277 [D loss: 0.843802, acc.: 40.62%] [G loss: 0.975660]\n",
      "278 [D loss: 0.618696, acc.: 56.25%] [G loss: 1.525623]\n",
      "279 [D loss: 0.714650, acc.: 46.88%] [G loss: 1.171832]\n",
      "280 [D loss: 0.673933, acc.: 54.69%] [G loss: 1.342051]\n",
      "281 [D loss: 0.667994, acc.: 57.81%] [G loss: 1.389653]\n",
      "282 [D loss: 0.766986, acc.: 46.88%] [G loss: 0.925942]\n",
      "283 [D loss: 0.732250, acc.: 46.88%] [G loss: 1.033415]\n",
      "284 [D loss: 0.627498, acc.: 59.38%] [G loss: 1.421077]\n",
      "285 [D loss: 1.012140, acc.: 23.44%] [G loss: 0.579116]\n",
      "286 [D loss: 0.647830, acc.: 54.69%] [G loss: 0.936624]\n",
      "287 [D loss: 0.707701, acc.: 54.69%] [G loss: 1.002136]\n",
      "288 [D loss: 0.845580, acc.: 37.50%] [G loss: 0.697443]\n",
      "289 [D loss: 0.630064, acc.: 54.69%] [G loss: 0.954398]\n",
      "290 [D loss: 0.796179, acc.: 34.38%] [G loss: 0.861298]\n",
      "291 [D loss: 0.723796, acc.: 45.31%] [G loss: 0.846710]\n",
      "292 [D loss: 0.754249, acc.: 42.19%] [G loss: 0.869929]\n",
      "293 [D loss: 0.728878, acc.: 48.44%] [G loss: 0.822671]\n",
      "294 [D loss: 0.736274, acc.: 43.75%] [G loss: 0.886484]\n",
      "295 [D loss: 0.696129, acc.: 43.75%] [G loss: 0.929414]\n",
      "296 [D loss: 0.795556, acc.: 31.25%] [G loss: 0.825495]\n",
      "297 [D loss: 0.743653, acc.: 50.00%] [G loss: 0.842390]\n",
      "298 [D loss: 0.686084, acc.: 51.56%] [G loss: 0.847855]\n",
      "299 [D loss: 0.716169, acc.: 50.00%] [G loss: 0.828061]\n",
      "300 [D loss: 0.763196, acc.: 35.94%] [G loss: 0.755716]\n",
      "301 [D loss: 0.715031, acc.: 42.19%] [G loss: 0.747956]\n",
      "302 [D loss: 0.677915, acc.: 48.44%] [G loss: 0.824992]\n",
      "303 [D loss: 0.719445, acc.: 50.00%] [G loss: 0.816094]\n",
      "304 [D loss: 0.738289, acc.: 42.19%] [G loss: 0.821370]\n",
      "305 [D loss: 0.735185, acc.: 42.19%] [G loss: 0.781568]\n",
      "306 [D loss: 0.735407, acc.: 40.62%] [G loss: 0.728707]\n",
      "307 [D loss: 0.695443, acc.: 46.88%] [G loss: 0.774113]\n",
      "308 [D loss: 0.687383, acc.: 48.44%] [G loss: 0.747000]\n",
      "309 [D loss: 0.720717, acc.: 42.19%] [G loss: 0.785449]\n",
      "310 [D loss: 0.755622, acc.: 32.81%] [G loss: 0.726732]\n",
      "311 [D loss: 0.753266, acc.: 42.19%] [G loss: 0.695203]\n",
      "312 [D loss: 0.702370, acc.: 48.44%] [G loss: 0.714612]\n",
      "313 [D loss: 0.709117, acc.: 50.00%] [G loss: 0.737090]\n",
      "314 [D loss: 0.703470, acc.: 46.88%] [G loss: 0.740862]\n",
      "315 [D loss: 0.750780, acc.: 40.62%] [G loss: 0.740292]\n",
      "316 [D loss: 0.674616, acc.: 48.44%] [G loss: 0.724253]\n",
      "317 [D loss: 0.713614, acc.: 43.75%] [G loss: 0.733171]\n",
      "318 [D loss: 0.686531, acc.: 53.12%] [G loss: 0.708242]\n",
      "319 [D loss: 0.702150, acc.: 40.62%] [G loss: 0.696813]\n",
      "320 [D loss: 0.710016, acc.: 40.62%] [G loss: 0.714830]\n",
      "321 [D loss: 0.652943, acc.: 51.56%] [G loss: 0.756055]\n",
      "322 [D loss: 0.686523, acc.: 48.44%] [G loss: 0.757737]\n",
      "323 [D loss: 0.699585, acc.: 46.88%] [G loss: 0.716835]\n",
      "324 [D loss: 0.721698, acc.: 39.06%] [G loss: 0.702526]\n",
      "325 [D loss: 0.691293, acc.: 45.31%] [G loss: 0.690766]\n",
      "326 [D loss: 0.637503, acc.: 62.50%] [G loss: 0.735239]\n",
      "327 [D loss: 0.713311, acc.: 42.19%] [G loss: 0.700004]\n",
      "328 [D loss: 0.695414, acc.: 43.75%] [G loss: 0.705278]\n",
      "329 [D loss: 0.688441, acc.: 50.00%] [G loss: 0.693631]\n",
      "330 [D loss: 0.666048, acc.: 50.00%] [G loss: 0.736632]\n",
      "331 [D loss: 0.710156, acc.: 34.38%] [G loss: 0.725956]\n",
      "332 [D loss: 0.693173, acc.: 45.31%] [G loss: 0.707122]\n",
      "333 [D loss: 0.663254, acc.: 51.56%] [G loss: 0.721407]\n",
      "334 [D loss: 0.697222, acc.: 43.75%] [G loss: 0.719761]\n",
      "335 [D loss: 0.669133, acc.: 48.44%] [G loss: 0.703580]\n",
      "336 [D loss: 0.690672, acc.: 43.75%] [G loss: 0.714301]\n",
      "337 [D loss: 0.639830, acc.: 53.12%] [G loss: 0.744238]\n",
      "338 [D loss: 0.684470, acc.: 54.69%] [G loss: 0.729288]\n",
      "339 [D loss: 0.689204, acc.: 48.44%] [G loss: 0.715746]\n",
      "340 [D loss: 0.698303, acc.: 45.31%] [G loss: 0.683725]\n",
      "341 [D loss: 0.662242, acc.: 51.56%] [G loss: 0.718451]\n",
      "342 [D loss: 0.669440, acc.: 53.12%] [G loss: 0.696303]\n",
      "343 [D loss: 0.650945, acc.: 57.81%] [G loss: 0.690348]\n",
      "344 [D loss: 0.705964, acc.: 40.62%] [G loss: 0.664186]\n",
      "345 [D loss: 0.694637, acc.: 45.31%] [G loss: 0.693515]\n",
      "346 [D loss: 0.689466, acc.: 48.44%] [G loss: 0.701752]\n",
      "347 [D loss: 0.670644, acc.: 50.00%] [G loss: 0.687778]\n",
      "348 [D loss: 0.686998, acc.: 42.19%] [G loss: 0.686233]\n",
      "349 [D loss: 0.684309, acc.: 40.62%] [G loss: 0.705719]\n",
      "350 [D loss: 0.652609, acc.: 48.44%] [G loss: 0.707455]\n",
      "351 [D loss: 0.674145, acc.: 45.31%] [G loss: 0.697624]\n",
      "352 [D loss: 0.659337, acc.: 43.75%] [G loss: 0.689484]\n",
      "353 [D loss: 0.674649, acc.: 54.69%] [G loss: 0.686375]\n",
      "354 [D loss: 0.663072, acc.: 48.44%] [G loss: 0.715568]\n",
      "355 [D loss: 0.667007, acc.: 42.19%] [G loss: 0.712511]\n",
      "356 [D loss: 0.657264, acc.: 57.81%] [G loss: 0.713928]\n",
      "357 [D loss: 0.694198, acc.: 40.62%] [G loss: 0.693459]\n",
      "358 [D loss: 0.682738, acc.: 43.75%] [G loss: 0.681921]\n",
      "359 [D loss: 0.672484, acc.: 46.88%] [G loss: 0.702224]\n",
      "360 [D loss: 0.668765, acc.: 50.00%] [G loss: 0.696258]\n",
      "361 [D loss: 0.692861, acc.: 46.88%] [G loss: 0.672866]\n",
      "362 [D loss: 0.636805, acc.: 57.81%] [G loss: 0.685229]\n",
      "363 [D loss: 0.665823, acc.: 46.88%] [G loss: 0.703253]\n",
      "364 [D loss: 0.658167, acc.: 51.56%] [G loss: 0.694446]\n",
      "365 [D loss: 0.691577, acc.: 42.19%] [G loss: 0.698241]\n",
      "366 [D loss: 0.661369, acc.: 50.00%] [G loss: 0.675719]\n",
      "367 [D loss: 0.662811, acc.: 50.00%] [G loss: 0.692161]\n",
      "368 [D loss: 0.665806, acc.: 45.31%] [G loss: 0.686781]\n",
      "369 [D loss: 0.677682, acc.: 42.19%] [G loss: 0.692757]\n",
      "370 [D loss: 0.674638, acc.: 46.88%] [G loss: 0.694282]\n",
      "371 [D loss: 0.669537, acc.: 48.44%] [G loss: 0.683165]\n",
      "372 [D loss: 0.682155, acc.: 46.88%] [G loss: 0.697999]\n",
      "373 [D loss: 0.669428, acc.: 50.00%] [G loss: 0.699506]\n",
      "374 [D loss: 0.669910, acc.: 45.31%] [G loss: 0.680874]\n",
      "375 [D loss: 0.690396, acc.: 46.88%] [G loss: 0.679709]\n",
      "376 [D loss: 0.659605, acc.: 50.00%] [G loss: 0.660601]\n",
      "377 [D loss: 0.648618, acc.: 56.25%] [G loss: 0.675631]\n",
      "378 [D loss: 0.672162, acc.: 51.56%] [G loss: 0.686152]\n",
      "379 [D loss: 0.673138, acc.: 56.25%] [G loss: 0.694752]\n",
      "380 [D loss: 0.659404, acc.: 54.69%] [G loss: 0.695506]\n",
      "381 [D loss: 0.677136, acc.: 48.44%] [G loss: 0.696721]\n",
      "382 [D loss: 0.654441, acc.: 54.69%] [G loss: 0.703191]\n",
      "383 [D loss: 0.658015, acc.: 57.81%] [G loss: 0.685877]\n",
      "384 [D loss: 0.662125, acc.: 54.69%] [G loss: 0.683843]\n",
      "385 [D loss: 0.675329, acc.: 48.44%] [G loss: 0.676591]\n",
      "386 [D loss: 0.675530, acc.: 48.44%] [G loss: 0.693242]\n",
      "387 [D loss: 0.651035, acc.: 56.25%] [G loss: 0.691772]\n",
      "388 [D loss: 0.655113, acc.: 54.69%] [G loss: 0.699874]\n",
      "389 [D loss: 0.666827, acc.: 59.38%] [G loss: 0.720294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 [D loss: 0.639724, acc.: 57.81%] [G loss: 0.706395]\n",
      "391 [D loss: 0.695111, acc.: 42.19%] [G loss: 0.703320]\n",
      "392 [D loss: 0.679359, acc.: 51.56%] [G loss: 0.707434]\n",
      "393 [D loss: 0.662217, acc.: 50.00%] [G loss: 0.676283]\n",
      "394 [D loss: 0.674956, acc.: 46.88%] [G loss: 0.670608]\n",
      "395 [D loss: 0.649650, acc.: 56.25%] [G loss: 0.678048]\n",
      "396 [D loss: 0.647616, acc.: 60.94%] [G loss: 0.675733]\n",
      "397 [D loss: 0.669179, acc.: 51.56%] [G loss: 0.682074]\n",
      "398 [D loss: 0.637482, acc.: 60.94%] [G loss: 0.676553]\n",
      "399 [D loss: 0.676193, acc.: 51.56%] [G loss: 0.693438]\n",
      "400 [D loss: 0.648049, acc.: 57.81%] [G loss: 0.691580]\n",
      "401 [D loss: 0.652733, acc.: 57.81%] [G loss: 0.710206]\n",
      "402 [D loss: 0.653347, acc.: 57.81%] [G loss: 0.712070]\n",
      "403 [D loss: 0.676325, acc.: 54.69%] [G loss: 0.695805]\n",
      "404 [D loss: 0.688043, acc.: 51.56%] [G loss: 0.690344]\n",
      "405 [D loss: 0.668038, acc.: 50.00%] [G loss: 0.682904]\n",
      "406 [D loss: 0.643668, acc.: 59.38%] [G loss: 0.693035]\n",
      "407 [D loss: 0.670073, acc.: 53.12%] [G loss: 0.681098]\n",
      "408 [D loss: 0.664645, acc.: 56.25%] [G loss: 0.700537]\n",
      "409 [D loss: 0.650640, acc.: 59.38%] [G loss: 0.713877]\n",
      "410 [D loss: 0.663160, acc.: 56.25%] [G loss: 0.699614]\n",
      "411 [D loss: 0.693170, acc.: 46.88%] [G loss: 0.677340]\n",
      "412 [D loss: 0.678305, acc.: 45.31%] [G loss: 0.695550]\n",
      "413 [D loss: 0.661056, acc.: 56.25%] [G loss: 0.714294]\n",
      "414 [D loss: 0.649553, acc.: 57.81%] [G loss: 0.735185]\n",
      "415 [D loss: 0.664524, acc.: 50.00%] [G loss: 0.740843]\n",
      "416 [D loss: 0.658389, acc.: 48.44%] [G loss: 0.711143]\n",
      "417 [D loss: 0.667383, acc.: 50.00%] [G loss: 0.678753]\n",
      "418 [D loss: 0.656673, acc.: 48.44%] [G loss: 0.677342]\n",
      "419 [D loss: 0.645581, acc.: 50.00%] [G loss: 0.676793]\n",
      "420 [D loss: 0.635166, acc.: 54.69%] [G loss: 0.694770]\n",
      "421 [D loss: 0.640876, acc.: 54.69%] [G loss: 0.707489]\n",
      "422 [D loss: 0.656178, acc.: 53.12%] [G loss: 0.707477]\n",
      "423 [D loss: 0.668606, acc.: 51.56%] [G loss: 0.698913]\n",
      "424 [D loss: 0.671617, acc.: 53.12%] [G loss: 0.677857]\n",
      "425 [D loss: 0.645272, acc.: 56.25%] [G loss: 0.708836]\n",
      "426 [D loss: 0.655177, acc.: 56.25%] [G loss: 0.712752]\n",
      "427 [D loss: 0.657809, acc.: 57.81%] [G loss: 0.699677]\n",
      "428 [D loss: 0.661469, acc.: 53.12%] [G loss: 0.702534]\n",
      "429 [D loss: 0.639249, acc.: 56.25%] [G loss: 0.711106]\n",
      "430 [D loss: 0.623699, acc.: 57.81%] [G loss: 0.715565]\n",
      "431 [D loss: 0.634621, acc.: 56.25%] [G loss: 0.721561]\n",
      "432 [D loss: 0.658172, acc.: 57.81%] [G loss: 0.715502]\n",
      "433 [D loss: 0.655646, acc.: 50.00%] [G loss: 0.717962]\n",
      "434 [D loss: 0.650153, acc.: 50.00%] [G loss: 0.697639]\n",
      "435 [D loss: 0.640876, acc.: 50.00%] [G loss: 0.712224]\n",
      "436 [D loss: 0.646268, acc.: 48.44%] [G loss: 0.700122]\n",
      "437 [D loss: 0.657369, acc.: 48.44%] [G loss: 0.699616]\n",
      "438 [D loss: 0.636140, acc.: 51.56%] [G loss: 0.700603]\n",
      "439 [D loss: 0.623967, acc.: 60.94%] [G loss: 0.692229]\n",
      "440 [D loss: 0.640580, acc.: 50.00%] [G loss: 0.694752]\n",
      "441 [D loss: 0.630010, acc.: 53.12%] [G loss: 0.703879]\n",
      "442 [D loss: 0.621422, acc.: 59.38%] [G loss: 0.704527]\n",
      "443 [D loss: 0.627240, acc.: 57.81%] [G loss: 0.727371]\n",
      "444 [D loss: 0.616936, acc.: 64.06%] [G loss: 0.714815]\n",
      "445 [D loss: 0.665080, acc.: 50.00%] [G loss: 0.716967]\n",
      "446 [D loss: 0.641747, acc.: 54.69%] [G loss: 0.718470]\n",
      "447 [D loss: 0.616793, acc.: 70.31%] [G loss: 0.691637]\n",
      "448 [D loss: 0.650783, acc.: 59.38%] [G loss: 0.697563]\n",
      "449 [D loss: 0.630455, acc.: 60.94%] [G loss: 0.691914]\n",
      "450 [D loss: 0.627950, acc.: 60.94%] [G loss: 0.686622]\n",
      "451 [D loss: 0.660533, acc.: 56.25%] [G loss: 0.690079]\n",
      "452 [D loss: 0.643454, acc.: 64.06%] [G loss: 0.690832]\n",
      "453 [D loss: 0.636868, acc.: 60.94%] [G loss: 0.701706]\n",
      "454 [D loss: 0.657613, acc.: 62.50%] [G loss: 0.718453]\n",
      "455 [D loss: 0.644773, acc.: 60.94%] [G loss: 0.724628]\n",
      "456 [D loss: 0.647233, acc.: 57.81%] [G loss: 0.710087]\n",
      "457 [D loss: 0.643242, acc.: 62.50%] [G loss: 0.720332]\n",
      "458 [D loss: 0.652303, acc.: 56.25%] [G loss: 0.712216]\n",
      "459 [D loss: 0.634796, acc.: 68.75%] [G loss: 0.722385]\n",
      "460 [D loss: 0.630400, acc.: 68.75%] [G loss: 0.717615]\n",
      "461 [D loss: 0.611208, acc.: 67.19%] [G loss: 0.722252]\n",
      "462 [D loss: 0.638213, acc.: 67.19%] [G loss: 0.724593]\n",
      "463 [D loss: 0.638945, acc.: 64.06%] [G loss: 0.778239]\n",
      "464 [D loss: 0.634771, acc.: 70.31%] [G loss: 0.790839]\n",
      "465 [D loss: 0.633747, acc.: 62.50%] [G loss: 0.807461]\n",
      "466 [D loss: 0.672188, acc.: 57.81%] [G loss: 0.759894]\n",
      "467 [D loss: 0.673197, acc.: 46.88%] [G loss: 0.762409]\n",
      "468 [D loss: 0.652856, acc.: 53.12%] [G loss: 0.754056]\n",
      "469 [D loss: 0.660268, acc.: 51.56%] [G loss: 0.709132]\n",
      "470 [D loss: 0.648860, acc.: 56.25%] [G loss: 0.728814]\n",
      "471 [D loss: 0.621791, acc.: 65.62%] [G loss: 0.779409]\n",
      "472 [D loss: 0.666495, acc.: 60.94%] [G loss: 0.772830]\n",
      "473 [D loss: 0.651755, acc.: 57.81%] [G loss: 0.752845]\n",
      "474 [D loss: 0.640602, acc.: 60.94%] [G loss: 0.750914]\n",
      "475 [D loss: 0.646393, acc.: 64.06%] [G loss: 0.725926]\n",
      "476 [D loss: 0.632660, acc.: 57.81%] [G loss: 0.756069]\n",
      "477 [D loss: 0.668381, acc.: 57.81%] [G loss: 0.735917]\n",
      "478 [D loss: 0.660032, acc.: 51.56%] [G loss: 0.747883]\n",
      "479 [D loss: 0.638664, acc.: 57.81%] [G loss: 0.768155]\n",
      "480 [D loss: 0.674995, acc.: 48.44%] [G loss: 0.755525]\n",
      "481 [D loss: 0.656648, acc.: 51.56%] [G loss: 0.727108]\n",
      "482 [D loss: 0.627099, acc.: 46.88%] [G loss: 0.738305]\n",
      "483 [D loss: 0.653206, acc.: 46.88%] [G loss: 0.733028]\n",
      "484 [D loss: 0.634844, acc.: 59.38%] [G loss: 0.714943]\n",
      "485 [D loss: 0.712912, acc.: 43.75%] [G loss: 0.725400]\n",
      "486 [D loss: 0.633350, acc.: 57.81%] [G loss: 0.732115]\n",
      "487 [D loss: 0.630029, acc.: 53.12%] [G loss: 0.731881]\n",
      "488 [D loss: 0.658390, acc.: 54.69%] [G loss: 0.730095]\n",
      "489 [D loss: 0.651490, acc.: 50.00%] [G loss: 0.726669]\n",
      "490 [D loss: 0.664128, acc.: 56.25%] [G loss: 0.726173]\n",
      "491 [D loss: 0.631301, acc.: 59.38%] [G loss: 0.733064]\n",
      "492 [D loss: 0.636322, acc.: 57.81%] [G loss: 0.722166]\n",
      "493 [D loss: 0.649100, acc.: 54.69%] [G loss: 0.742630]\n",
      "494 [D loss: 0.671407, acc.: 51.56%] [G loss: 0.768117]\n",
      "495 [D loss: 0.646573, acc.: 62.50%] [G loss: 0.792235]\n",
      "496 [D loss: 0.611325, acc.: 71.88%] [G loss: 0.783784]\n",
      "497 [D loss: 0.635337, acc.: 56.25%] [G loss: 0.758993]\n",
      "498 [D loss: 0.639887, acc.: 64.06%] [G loss: 0.760880]\n",
      "499 [D loss: 0.610029, acc.: 64.06%] [G loss: 0.771245]\n",
      "500 [D loss: 0.618245, acc.: 64.06%] [G loss: 0.812582]\n",
      "501 [D loss: 0.643207, acc.: 54.69%] [G loss: 0.801861]\n",
      "502 [D loss: 0.636808, acc.: 62.50%] [G loss: 0.775983]\n",
      "503 [D loss: 0.621777, acc.: 62.50%] [G loss: 0.770597]\n",
      "504 [D loss: 0.614930, acc.: 65.62%] [G loss: 0.769546]\n",
      "505 [D loss: 0.655911, acc.: 54.69%] [G loss: 0.721204]\n",
      "506 [D loss: 0.633173, acc.: 64.06%] [G loss: 0.732380]\n",
      "507 [D loss: 0.660919, acc.: 53.12%] [G loss: 0.740102]\n",
      "508 [D loss: 0.658854, acc.: 62.50%] [G loss: 0.720315]\n",
      "509 [D loss: 0.668913, acc.: 57.81%] [G loss: 0.704701]\n",
      "510 [D loss: 0.667094, acc.: 48.44%] [G loss: 0.718567]\n",
      "511 [D loss: 0.638614, acc.: 64.06%] [G loss: 0.728026]\n",
      "512 [D loss: 0.661683, acc.: 54.69%] [G loss: 0.726014]\n",
      "513 [D loss: 0.660177, acc.: 53.12%] [G loss: 0.738997]\n",
      "514 [D loss: 0.652365, acc.: 60.94%] [G loss: 0.717262]\n",
      "515 [D loss: 0.648618, acc.: 59.38%] [G loss: 0.751349]\n",
      "516 [D loss: 0.632331, acc.: 59.38%] [G loss: 0.718458]\n",
      "517 [D loss: 0.698061, acc.: 53.12%] [G loss: 0.720071]\n",
      "518 [D loss: 0.670213, acc.: 53.12%] [G loss: 0.727935]\n",
      "519 [D loss: 0.699156, acc.: 62.50%] [G loss: 0.747173]\n",
      "520 [D loss: 0.690936, acc.: 53.12%] [G loss: 0.793375]\n",
      "521 [D loss: 0.655373, acc.: 64.06%] [G loss: 0.772727]\n",
      "522 [D loss: 0.677213, acc.: 54.69%] [G loss: 0.750633]\n",
      "523 [D loss: 0.660859, acc.: 59.38%] [G loss: 0.774040]\n",
      "524 [D loss: 0.633116, acc.: 67.19%] [G loss: 0.835031]\n",
      "525 [D loss: 0.674779, acc.: 53.12%] [G loss: 0.741896]\n",
      "526 [D loss: 0.650971, acc.: 59.38%] [G loss: 0.757820]\n",
      "527 [D loss: 0.651074, acc.: 68.75%] [G loss: 0.770370]\n",
      "528 [D loss: 0.689905, acc.: 48.44%] [G loss: 0.782438]\n",
      "529 [D loss: 0.668965, acc.: 64.06%] [G loss: 0.764321]\n",
      "530 [D loss: 0.649014, acc.: 67.19%] [G loss: 0.759687]\n",
      "531 [D loss: 0.647158, acc.: 60.94%] [G loss: 0.742179]\n",
      "532 [D loss: 0.649820, acc.: 62.50%] [G loss: 0.759362]\n",
      "533 [D loss: 0.647536, acc.: 57.81%] [G loss: 0.717594]\n",
      "534 [D loss: 0.643155, acc.: 57.81%] [G loss: 0.739664]\n",
      "535 [D loss: 0.641315, acc.: 56.25%] [G loss: 0.740503]\n",
      "536 [D loss: 0.645469, acc.: 56.25%] [G loss: 0.756674]\n",
      "537 [D loss: 0.616833, acc.: 60.94%] [G loss: 0.764063]\n",
      "538 [D loss: 0.635952, acc.: 56.25%] [G loss: 0.764389]\n",
      "539 [D loss: 0.626613, acc.: 60.94%] [G loss: 0.753356]\n",
      "540 [D loss: 0.635801, acc.: 53.12%] [G loss: 0.752176]\n",
      "541 [D loss: 0.627464, acc.: 62.50%] [G loss: 0.757281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542 [D loss: 0.627530, acc.: 65.62%] [G loss: 0.761764]\n",
      "543 [D loss: 0.617970, acc.: 59.38%] [G loss: 0.775548]\n",
      "544 [D loss: 0.621434, acc.: 70.31%] [G loss: 0.772352]\n",
      "545 [D loss: 0.602991, acc.: 68.75%] [G loss: 0.774311]\n",
      "546 [D loss: 0.605501, acc.: 65.62%] [G loss: 0.763804]\n",
      "547 [D loss: 0.624878, acc.: 65.62%] [G loss: 0.749131]\n",
      "548 [D loss: 0.616234, acc.: 60.94%] [G loss: 0.762538]\n",
      "549 [D loss: 0.621416, acc.: 71.88%] [G loss: 0.763877]\n",
      "550 [D loss: 0.615601, acc.: 60.94%] [G loss: 0.757234]\n",
      "551 [D loss: 0.615445, acc.: 70.31%] [G loss: 0.760915]\n",
      "552 [D loss: 0.604259, acc.: 71.88%] [G loss: 0.783691]\n",
      "553 [D loss: 0.615689, acc.: 67.19%] [G loss: 0.759216]\n",
      "554 [D loss: 0.590963, acc.: 73.44%] [G loss: 0.766441]\n",
      "555 [D loss: 0.615497, acc.: 68.75%] [G loss: 0.758107]\n",
      "556 [D loss: 0.599690, acc.: 67.19%] [G loss: 0.769679]\n",
      "557 [D loss: 0.604621, acc.: 67.19%] [G loss: 0.783652]\n",
      "558 [D loss: 0.605167, acc.: 70.31%] [G loss: 0.775386]\n",
      "559 [D loss: 0.571647, acc.: 78.12%] [G loss: 0.782336]\n",
      "560 [D loss: 0.565127, acc.: 76.56%] [G loss: 0.788818]\n",
      "561 [D loss: 0.598521, acc.: 71.88%] [G loss: 0.787746]\n",
      "562 [D loss: 0.601876, acc.: 75.00%] [G loss: 0.790811]\n",
      "563 [D loss: 0.604696, acc.: 73.44%] [G loss: 0.780691]\n",
      "564 [D loss: 0.593169, acc.: 71.88%] [G loss: 0.789857]\n",
      "565 [D loss: 0.571866, acc.: 81.25%] [G loss: 0.766745]\n",
      "566 [D loss: 0.564172, acc.: 68.75%] [G loss: 0.773817]\n",
      "567 [D loss: 0.610895, acc.: 67.19%] [G loss: 0.757341]\n",
      "568 [D loss: 0.612976, acc.: 67.19%] [G loss: 0.744195]\n",
      "569 [D loss: 0.666569, acc.: 57.81%] [G loss: 0.767905]\n",
      "570 [D loss: 0.673396, acc.: 54.69%] [G loss: 0.808006]\n",
      "571 [D loss: 0.637130, acc.: 60.94%] [G loss: 0.818636]\n",
      "572 [D loss: 0.619425, acc.: 65.62%] [G loss: 0.828909]\n",
      "573 [D loss: 0.690258, acc.: 59.38%] [G loss: 0.790857]\n",
      "574 [D loss: 0.639568, acc.: 54.69%] [G loss: 0.864246]\n",
      "575 [D loss: 0.631062, acc.: 68.75%] [G loss: 0.885305]\n",
      "576 [D loss: 0.619761, acc.: 71.88%] [G loss: 0.899803]\n",
      "577 [D loss: 0.624360, acc.: 65.62%] [G loss: 0.839936]\n",
      "578 [D loss: 0.669869, acc.: 53.12%] [G loss: 0.782112]\n",
      "579 [D loss: 0.637940, acc.: 57.81%] [G loss: 0.782936]\n",
      "580 [D loss: 0.618078, acc.: 65.62%] [G loss: 0.778178]\n",
      "581 [D loss: 0.620832, acc.: 60.94%] [G loss: 0.798144]\n",
      "582 [D loss: 0.612423, acc.: 70.31%] [G loss: 0.758752]\n",
      "583 [D loss: 0.637868, acc.: 60.94%] [G loss: 0.793545]\n",
      "584 [D loss: 0.646558, acc.: 53.12%] [G loss: 0.785532]\n",
      "585 [D loss: 0.620626, acc.: 60.94%] [G loss: 0.761864]\n",
      "586 [D loss: 0.609840, acc.: 62.50%] [G loss: 0.762556]\n",
      "587 [D loss: 0.633724, acc.: 62.50%] [G loss: 0.786828]\n",
      "588 [D loss: 0.609514, acc.: 71.88%] [G loss: 0.799064]\n",
      "589 [D loss: 0.618947, acc.: 67.19%] [G loss: 0.838285]\n",
      "590 [D loss: 0.579595, acc.: 78.12%] [G loss: 0.857410]\n",
      "591 [D loss: 0.642001, acc.: 65.62%] [G loss: 0.822379]\n",
      "592 [D loss: 0.634747, acc.: 67.19%] [G loss: 0.760850]\n",
      "593 [D loss: 0.635638, acc.: 64.06%] [G loss: 0.761997]\n",
      "594 [D loss: 0.646239, acc.: 59.38%] [G loss: 0.771561]\n",
      "595 [D loss: 0.598529, acc.: 70.31%] [G loss: 0.762034]\n",
      "596 [D loss: 0.629139, acc.: 64.06%] [G loss: 0.740737]\n",
      "597 [D loss: 0.653798, acc.: 62.50%] [G loss: 0.755896]\n",
      "598 [D loss: 0.645372, acc.: 65.62%] [G loss: 0.788497]\n",
      "599 [D loss: 0.620143, acc.: 73.44%] [G loss: 0.793786]\n",
      "600 [D loss: 0.623667, acc.: 67.19%] [G loss: 0.789480]\n",
      "601 [D loss: 0.616305, acc.: 75.00%] [G loss: 0.767895]\n",
      "602 [D loss: 0.629515, acc.: 64.06%] [G loss: 0.756098]\n",
      "603 [D loss: 0.631172, acc.: 59.38%] [G loss: 0.759975]\n",
      "604 [D loss: 0.634498, acc.: 59.38%] [G loss: 0.806669]\n",
      "605 [D loss: 0.646643, acc.: 60.94%] [G loss: 0.813161]\n",
      "606 [D loss: 0.664307, acc.: 50.00%] [G loss: 0.833909]\n",
      "607 [D loss: 0.651862, acc.: 64.06%] [G loss: 0.829523]\n",
      "608 [D loss: 0.600326, acc.: 75.00%] [G loss: 0.827802]\n",
      "609 [D loss: 0.628079, acc.: 71.88%] [G loss: 0.811461]\n",
      "610 [D loss: 0.635904, acc.: 67.19%] [G loss: 0.861062]\n",
      "611 [D loss: 0.648909, acc.: 59.38%] [G loss: 0.860199]\n",
      "612 [D loss: 0.640007, acc.: 60.94%] [G loss: 0.873675]\n",
      "613 [D loss: 0.628724, acc.: 67.19%] [G loss: 0.827613]\n",
      "614 [D loss: 0.608406, acc.: 70.31%] [G loss: 0.791328]\n",
      "615 [D loss: 0.628855, acc.: 62.50%] [G loss: 0.797429]\n",
      "616 [D loss: 0.645549, acc.: 57.81%] [G loss: 0.799610]\n",
      "617 [D loss: 0.651853, acc.: 67.19%] [G loss: 0.797273]\n",
      "618 [D loss: 0.619403, acc.: 70.31%] [G loss: 0.771760]\n",
      "619 [D loss: 0.635206, acc.: 67.19%] [G loss: 0.813932]\n",
      "620 [D loss: 0.624115, acc.: 60.94%] [G loss: 0.811132]\n",
      "621 [D loss: 0.594247, acc.: 65.62%] [G loss: 0.826578]\n",
      "622 [D loss: 0.658482, acc.: 62.50%] [G loss: 0.810847]\n",
      "623 [D loss: 0.597784, acc.: 62.50%] [G loss: 0.847264]\n",
      "624 [D loss: 0.612845, acc.: 68.75%] [G loss: 0.825369]\n",
      "625 [D loss: 0.609790, acc.: 75.00%] [G loss: 0.826414]\n",
      "626 [D loss: 0.608550, acc.: 64.06%] [G loss: 0.820174]\n",
      "627 [D loss: 0.633393, acc.: 65.62%] [G loss: 0.767009]\n",
      "628 [D loss: 0.619515, acc.: 68.75%] [G loss: 0.800510]\n",
      "629 [D loss: 0.621020, acc.: 75.00%] [G loss: 0.764179]\n",
      "630 [D loss: 0.599571, acc.: 65.62%] [G loss: 0.791886]\n",
      "631 [D loss: 0.616960, acc.: 67.19%] [G loss: 0.811251]\n",
      "632 [D loss: 0.635853, acc.: 76.56%] [G loss: 0.794552]\n",
      "633 [D loss: 0.647548, acc.: 57.81%] [G loss: 0.808156]\n",
      "634 [D loss: 0.652418, acc.: 57.81%] [G loss: 0.773770]\n",
      "635 [D loss: 0.599354, acc.: 70.31%] [G loss: 0.817671]\n",
      "636 [D loss: 0.628549, acc.: 65.62%] [G loss: 0.828314]\n",
      "637 [D loss: 0.652600, acc.: 64.06%] [G loss: 0.803047]\n",
      "638 [D loss: 0.608811, acc.: 71.88%] [G loss: 0.819687]\n",
      "639 [D loss: 0.609145, acc.: 67.19%] [G loss: 0.826389]\n",
      "640 [D loss: 0.625827, acc.: 64.06%] [G loss: 0.850641]\n",
      "641 [D loss: 0.638318, acc.: 57.81%] [G loss: 0.828441]\n",
      "642 [D loss: 0.644172, acc.: 59.38%] [G loss: 0.814122]\n",
      "643 [D loss: 0.610814, acc.: 70.31%] [G loss: 0.799442]\n",
      "644 [D loss: 0.612881, acc.: 70.31%] [G loss: 0.805354]\n",
      "645 [D loss: 0.637903, acc.: 68.75%] [G loss: 0.781413]\n",
      "646 [D loss: 0.632054, acc.: 67.19%] [G loss: 0.760942]\n",
      "647 [D loss: 0.628232, acc.: 59.38%] [G loss: 0.817972]\n",
      "648 [D loss: 0.633798, acc.: 64.06%] [G loss: 0.795340]\n",
      "649 [D loss: 0.681035, acc.: 54.69%] [G loss: 0.774052]\n",
      "650 [D loss: 0.641089, acc.: 60.94%] [G loss: 0.792077]\n",
      "651 [D loss: 0.640152, acc.: 64.06%] [G loss: 0.826158]\n",
      "652 [D loss: 0.672441, acc.: 51.56%] [G loss: 0.811761]\n",
      "653 [D loss: 0.644629, acc.: 59.38%] [G loss: 0.802595]\n",
      "654 [D loss: 0.623722, acc.: 65.62%] [G loss: 0.836182]\n",
      "655 [D loss: 0.655139, acc.: 60.94%] [G loss: 0.782161]\n",
      "656 [D loss: 0.601526, acc.: 70.31%] [G loss: 0.804144]\n",
      "657 [D loss: 0.633626, acc.: 62.50%] [G loss: 0.796473]\n",
      "658 [D loss: 0.690210, acc.: 51.56%] [G loss: 0.764875]\n",
      "659 [D loss: 0.659554, acc.: 60.94%] [G loss: 0.805874]\n",
      "660 [D loss: 0.615055, acc.: 70.31%] [G loss: 0.786296]\n",
      "661 [D loss: 0.668736, acc.: 57.81%] [G loss: 0.766858]\n",
      "662 [D loss: 0.646955, acc.: 56.25%] [G loss: 0.799932]\n",
      "663 [D loss: 0.654249, acc.: 53.12%] [G loss: 0.824282]\n",
      "664 [D loss: 0.612579, acc.: 73.44%] [G loss: 0.835052]\n",
      "665 [D loss: 0.647237, acc.: 57.81%] [G loss: 0.851390]\n",
      "666 [D loss: 0.650895, acc.: 60.94%] [G loss: 0.818218]\n",
      "667 [D loss: 0.674759, acc.: 50.00%] [G loss: 0.806144]\n",
      "668 [D loss: 0.662297, acc.: 57.81%] [G loss: 0.856641]\n",
      "669 [D loss: 0.685710, acc.: 50.00%] [G loss: 0.821115]\n",
      "670 [D loss: 0.675748, acc.: 50.00%] [G loss: 0.760331]\n",
      "671 [D loss: 0.647456, acc.: 60.94%] [G loss: 0.735133]\n",
      "672 [D loss: 0.677567, acc.: 59.38%] [G loss: 0.728756]\n",
      "673 [D loss: 0.637900, acc.: 59.38%] [G loss: 0.725077]\n",
      "674 [D loss: 0.666264, acc.: 53.12%] [G loss: 0.757997]\n",
      "675 [D loss: 0.676719, acc.: 50.00%] [G loss: 0.782940]\n",
      "676 [D loss: 0.664816, acc.: 57.81%] [G loss: 0.819768]\n",
      "677 [D loss: 0.672747, acc.: 54.69%] [G loss: 0.799438]\n",
      "678 [D loss: 0.658051, acc.: 54.69%] [G loss: 0.804337]\n",
      "679 [D loss: 0.671851, acc.: 60.94%] [G loss: 0.805452]\n",
      "680 [D loss: 0.609430, acc.: 75.00%] [G loss: 0.828515]\n",
      "681 [D loss: 0.683317, acc.: 54.69%] [G loss: 0.768942]\n",
      "682 [D loss: 0.648807, acc.: 57.81%] [G loss: 0.769863]\n",
      "683 [D loss: 0.669762, acc.: 57.81%] [G loss: 0.827687]\n",
      "684 [D loss: 0.673040, acc.: 54.69%] [G loss: 0.802499]\n",
      "685 [D loss: 0.676494, acc.: 51.56%] [G loss: 0.813497]\n",
      "686 [D loss: 0.695904, acc.: 51.56%] [G loss: 0.794836]\n",
      "687 [D loss: 0.664309, acc.: 57.81%] [G loss: 0.790897]\n",
      "688 [D loss: 0.675793, acc.: 51.56%] [G loss: 0.778743]\n",
      "689 [D loss: 0.677332, acc.: 53.12%] [G loss: 0.800944]\n",
      "690 [D loss: 0.681007, acc.: 56.25%] [G loss: 0.765387]\n",
      "691 [D loss: 0.639223, acc.: 62.50%] [G loss: 0.740530]\n",
      "692 [D loss: 0.698813, acc.: 51.56%] [G loss: 0.728224]\n",
      "693 [D loss: 0.673930, acc.: 57.81%] [G loss: 0.765565]\n",
      "694 [D loss: 0.668681, acc.: 59.38%] [G loss: 0.786460]\n",
      "695 [D loss: 0.696758, acc.: 45.31%] [G loss: 0.809764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696 [D loss: 0.639348, acc.: 62.50%] [G loss: 0.807596]\n",
      "697 [D loss: 0.664019, acc.: 56.25%] [G loss: 0.776387]\n",
      "698 [D loss: 0.642132, acc.: 64.06%] [G loss: 0.735452]\n",
      "699 [D loss: 0.640685, acc.: 64.06%] [G loss: 0.765397]\n",
      "700 [D loss: 0.646012, acc.: 56.25%] [G loss: 0.778296]\n",
      "701 [D loss: 0.645688, acc.: 60.94%] [G loss: 0.785332]\n",
      "702 [D loss: 0.649827, acc.: 64.06%] [G loss: 0.749190]\n",
      "703 [D loss: 0.656409, acc.: 57.81%] [G loss: 0.767682]\n",
      "704 [D loss: 0.668623, acc.: 54.69%] [G loss: 0.774638]\n",
      "705 [D loss: 0.662193, acc.: 57.81%] [G loss: 0.789276]\n",
      "706 [D loss: 0.625194, acc.: 64.06%] [G loss: 0.798191]\n",
      "707 [D loss: 0.657458, acc.: 54.69%] [G loss: 0.792358]\n",
      "708 [D loss: 0.652179, acc.: 59.38%] [G loss: 0.759228]\n",
      "709 [D loss: 0.612780, acc.: 78.12%] [G loss: 0.754460]\n",
      "710 [D loss: 0.649115, acc.: 56.25%] [G loss: 0.736305]\n",
      "711 [D loss: 0.654281, acc.: 68.75%] [G loss: 0.732031]\n",
      "712 [D loss: 0.604130, acc.: 64.06%] [G loss: 0.761815]\n",
      "713 [D loss: 0.640201, acc.: 54.69%] [G loss: 0.774785]\n",
      "714 [D loss: 0.637035, acc.: 67.19%] [G loss: 0.770622]\n",
      "715 [D loss: 0.645599, acc.: 64.06%] [G loss: 0.784046]\n",
      "716 [D loss: 0.650958, acc.: 54.69%] [G loss: 0.762126]\n",
      "717 [D loss: 0.678195, acc.: 51.56%] [G loss: 0.770027]\n",
      "718 [D loss: 0.636962, acc.: 60.94%] [G loss: 0.762663]\n",
      "719 [D loss: 0.618398, acc.: 65.62%] [G loss: 0.807598]\n",
      "720 [D loss: 0.610488, acc.: 71.88%] [G loss: 0.803795]\n",
      "721 [D loss: 0.621516, acc.: 67.19%] [G loss: 0.759993]\n",
      "722 [D loss: 0.652540, acc.: 62.50%] [G loss: 0.771654]\n",
      "723 [D loss: 0.640078, acc.: 62.50%] [G loss: 0.770171]\n",
      "724 [D loss: 0.615835, acc.: 68.75%] [G loss: 0.815446]\n",
      "725 [D loss: 0.620589, acc.: 76.56%] [G loss: 0.807383]\n",
      "726 [D loss: 0.639145, acc.: 65.62%] [G loss: 0.801157]\n",
      "727 [D loss: 0.665495, acc.: 64.06%] [G loss: 0.721456]\n",
      "728 [D loss: 0.621908, acc.: 67.19%] [G loss: 0.773398]\n",
      "729 [D loss: 0.633711, acc.: 59.38%] [G loss: 0.818674]\n",
      "730 [D loss: 0.658879, acc.: 59.38%] [G loss: 0.785141]\n",
      "731 [D loss: 0.622958, acc.: 68.75%] [G loss: 0.753091]\n",
      "732 [D loss: 0.624578, acc.: 64.06%] [G loss: 0.770686]\n",
      "733 [D loss: 0.635259, acc.: 71.88%] [G loss: 0.779351]\n",
      "734 [D loss: 0.594261, acc.: 71.88%] [G loss: 0.816552]\n",
      "735 [D loss: 0.670866, acc.: 46.88%] [G loss: 0.783771]\n",
      "736 [D loss: 0.631509, acc.: 67.19%] [G loss: 0.750507]\n",
      "737 [D loss: 0.631072, acc.: 60.94%] [G loss: 0.790171]\n",
      "738 [D loss: 0.645052, acc.: 60.94%] [G loss: 0.760191]\n",
      "739 [D loss: 0.632840, acc.: 62.50%] [G loss: 0.778631]\n",
      "740 [D loss: 0.611706, acc.: 71.88%] [G loss: 0.788906]\n",
      "741 [D loss: 0.615493, acc.: 70.31%] [G loss: 0.796163]\n",
      "742 [D loss: 0.622303, acc.: 67.19%] [G loss: 0.783623]\n",
      "743 [D loss: 0.670868, acc.: 67.19%] [G loss: 0.731839]\n",
      "744 [D loss: 0.612606, acc.: 67.19%] [G loss: 0.767599]\n",
      "745 [D loss: 0.627811, acc.: 71.88%] [G loss: 0.799648]\n",
      "746 [D loss: 0.627435, acc.: 70.31%] [G loss: 0.812534]\n",
      "747 [D loss: 0.640616, acc.: 65.62%] [G loss: 0.793689]\n",
      "748 [D loss: 0.626429, acc.: 62.50%] [G loss: 0.778134]\n",
      "749 [D loss: 0.606475, acc.: 75.00%] [G loss: 0.740329]\n"
     ]
    }
   ],
   "source": [
    "GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
